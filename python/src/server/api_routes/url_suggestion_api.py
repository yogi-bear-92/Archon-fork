"""
API endpoints for URL suggestion management and user interaction.

This module provides endpoints for users to review, approve, reject, and manage
URL suggestions generated by the automatic URL detection system.
"""

import logging
from datetime import datetime
from typing import List, Optional

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from ..config.logfire_config import get_logger, safe_span, safe_set_attribute
from ..services.client_manager import get_supabase_client
from ..services.url_detection_service import get_url_detection_service
from ..agents.url_decision_agent import get_url_decision_agent

logger = get_logger(__name__)

router = APIRouter(prefix="/api/url-suggestions", tags=["URL Suggestions"])


# Request/Response Models

class URLSuggestionResponse(BaseModel):
    """Response model for URL suggestion."""
    id: str
    url: str
    domain: str
    relevance_score: float
    content_quality_score: float
    domain_reputation_score: float
    overall_score: float
    reasoning: str
    recommended_action: str
    source_context: str
    tags: List[str]
    status: str
    detected_at: str
    created_at: str


class URLSuggestionBatchResponse(BaseModel):
    """Response model for batch of URL suggestions."""
    suggestions: List[URLSuggestionResponse]
    total_count: int
    pending_count: int
    high_score_count: int


class ApproveURLRequest(BaseModel):
    """Request model for approving URL suggestions."""
    suggestion_ids: List[str] = Field(..., description="List of suggestion IDs to approve")
    add_tags: Optional[List[str]] = Field(default=None, description="Additional tags to add")
    source_type: Optional[str] = Field(default="user_approved", description="Source type for crawling")


class RejectURLRequest(BaseModel):
    """Request model for rejecting URL suggestions."""
    suggestion_ids: List[str] = Field(..., description="List of suggestion IDs to reject")
    reason: Optional[str] = Field(default=None, description="Reason for rejection")


class URLAnalysisRequest(BaseModel):
    """Request model for manual URL analysis."""
    url: str = Field(..., description="URL to analyze")
    context: Optional[str] = Field(default="", description="Context about the URL")
    force_reanalysis: Optional[bool] = Field(default=False, description="Force re-analysis even if cached")


class URLDetectionSettingsResponse(BaseModel):
    """Response model for URL detection settings."""
    enabled: bool
    auto_add_threshold: float
    suggest_threshold: float
    ignore_threshold: float
    excluded_domains: List[str]
    preferred_domains: List[str]


class UpdateSettingsRequest(BaseModel):
    """Request model for updating URL detection settings."""
    enabled: Optional[bool] = None
    auto_add_threshold: Optional[float] = Field(None, ge=0.0, le=1.0)
    suggest_threshold: Optional[float] = Field(None, ge=0.0, le=1.0)
    ignore_threshold: Optional[float] = Field(None, ge=0.0, le=1.0)
    excluded_domains: Optional[List[str]] = None
    preferred_domains: Optional[List[str]] = None


# API Endpoints

@router.get("/", response_model=URLSuggestionBatchResponse)
async def get_url_suggestions(
    status: str = Query("pending", description="Filter by status: pending, approved, rejected, expired"),
    limit: int = Query(20, ge=1, le=100, description="Maximum number of suggestions to return"),
    offset: int = Query(0, ge=0, description="Number of suggestions to skip"),
    min_score: float = Query(0.0, ge=0.0, le=1.0, description="Minimum overall score filter"),
    domain: Optional[str] = Query(None, description="Filter by domain"),
    tags: Optional[str] = Query(None, description="Filter by tags (comma-separated)")
):
    """
    Get URL suggestions with filtering and pagination.
    
    Returns a list of URL suggestions that match the specified filters,
    ordered by overall score (highest first) and then by detection date.
    """
    with safe_span("get_url_suggestions") as span:
        safe_set_attribute(span, "status", status)
        safe_set_attribute(span, "limit", limit)
        safe_set_attribute(span, "offset", offset)
        
        try:
            client = get_supabase_client()
            
            # Build the query
            query = client.table("url_suggestions").select("*")
            
            # Apply filters
            if status:
                query = query.eq("status", status)
            
            if min_score > 0:
                query = query.gte("overall_score", min_score)
            
            if domain:
                query = query.eq("domain", domain)
            
            if tags:
                tag_list = [tag.strip() for tag in tags.split(",")]
                for tag in tag_list:
                    query = query.contains("tags", [tag])
            
            # Get total count for pagination
            count_result = query.execute()
            total_count = len(count_result.data) if count_result.data else 0
            
            # Get paginated results
            result = query.order("overall_score", desc=True).order(
                "detected_at", desc=True
            ).range(offset, offset + limit - 1).execute()
            
            suggestions = []
            if result.data:
                for row in result.data:
                    suggestions.append(URLSuggestionResponse(
                        id=row["id"],
                        url=row["url"],
                        domain=row["domain"],
                        relevance_score=row["relevance_score"],
                        content_quality_score=row["content_quality_score"],
                        domain_reputation_score=row["domain_reputation_score"],
                        overall_score=row["overall_score"],
                        reasoning=row["reasoning"],
                        recommended_action=row["recommended_action"],
                        source_context=row["source_context"],
                        tags=row.get("tags", []),
                        status=row["status"],
                        detected_at=row["detected_at"],
                        created_at=row["created_at"]
                    ))
            
            # Get additional counts
            pending_result = client.table("url_suggestions").select(
                "id"
            ).eq("status", "pending").execute()
            pending_count = len(pending_result.data) if pending_result.data else 0
            
            high_score_result = client.table("url_suggestions").select(
                "id"
            ).eq("status", "pending").gte("overall_score", 0.8).execute()
            high_score_count = len(high_score_result.data) if high_score_result.data else 0
            
            safe_set_attribute(span, "suggestions_found", len(suggestions))
            safe_set_attribute(span, "total_count", total_count)
            
            return URLSuggestionBatchResponse(
                suggestions=suggestions,
                total_count=total_count,
                pending_count=pending_count,
                high_score_count=high_score_count
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error getting URL suggestions: {e}")
            raise HTTPException(status_code=500, detail="Failed to fetch URL suggestions")


@router.post("/approve")
async def approve_url_suggestions(request: ApproveURLRequest):
    """
    Approve one or more URL suggestions and add them to the knowledge base.
    
    This will trigger crawling of the approved URLs and update their status.
    """
    with safe_span("approve_url_suggestions") as span:
        safe_set_attribute(span, "suggestion_count", len(request.suggestion_ids))
        
        try:
            url_detection_service = get_url_detection_service()
            client = get_supabase_client()
            
            approved_urls = []
            failed_approvals = []
            
            for suggestion_id in request.suggestion_ids:
                try:
                    # Get suggestion details
                    suggestion_result = client.table("url_suggestions").select(
                        "*"
                    ).eq("id", suggestion_id).single().execute()
                    
                    if not suggestion_result.data:
                        failed_approvals.append({"id": suggestion_id, "error": "Suggestion not found"})
                        continue
                    
                    suggestion = suggestion_result.data
                    
                    # Check if still pending
                    if suggestion["status"] != "pending":
                        failed_approvals.append({
                            "id": suggestion_id, 
                            "error": f"Suggestion status is {suggestion['status']}, not pending"
                        })
                        continue
                    
                    # Approve the suggestion
                    success = await url_detection_service.approve_url_suggestion(suggestion_id)
                    
                    if success:
                        approved_urls.append({
                            "id": suggestion_id,
                            "url": suggestion["url"],
                            "tags": suggestion.get("tags", []) + (request.add_tags or [])
                        })
                        
                        # Learn from user feedback
                        url_decision_agent = get_url_decision_agent()
                        await url_decision_agent.learn_from_user_feedback(
                            suggestion["url"],
                            suggestion["recommended_action"],
                            "approved",
                            "User manually approved suggestion"
                        )
                    else:
                        failed_approvals.append({"id": suggestion_id, "error": "Failed to approve"})
                        
                except Exception as e:
                    logger.error(f"‚ùå Error approving suggestion {suggestion_id}: {e}")
                    failed_approvals.append({"id": suggestion_id, "error": str(e)})
            
            safe_set_attribute(span, "approved_count", len(approved_urls))
            safe_set_attribute(span, "failed_count", len(failed_approvals))
            
            return {
                "success": True,
                "approved_count": len(approved_urls),
                "failed_count": len(failed_approvals),
                "approved_urls": approved_urls,
                "failed_approvals": failed_approvals,
                "message": f"Approved {len(approved_urls)} URLs, {len(failed_approvals)} failed"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in approval process: {e}")
            raise HTTPException(status_code=500, detail="Failed to approve URL suggestions")


@router.post("/reject")
async def reject_url_suggestions(request: RejectURLRequest):
    """
    Reject one or more URL suggestions.
    
    This will update their status to rejected and optionally record the reason.
    """
    with safe_span("reject_url_suggestions") as span:
        safe_set_attribute(span, "suggestion_count", len(request.suggestion_ids))
        
        try:
            client = get_supabase_client()
            url_decision_agent = get_url_decision_agent()
            
            rejected_urls = []
            failed_rejections = []
            
            for suggestion_id in request.suggestion_ids:
                try:
                    # Get suggestion details
                    suggestion_result = client.table("url_suggestions").select(
                        "*"
                    ).eq("id", suggestion_id).single().execute()
                    
                    if not suggestion_result.data:
                        failed_rejections.append({"id": suggestion_id, "error": "Suggestion not found"})
                        continue
                    
                    suggestion = suggestion_result.data
                    
                    # Check if still pending
                    if suggestion["status"] != "pending":
                        failed_rejections.append({
                            "id": suggestion_id, 
                            "error": f"Suggestion status is {suggestion['status']}, not pending"
                        })
                        continue
                    
                    # Update suggestion status
                    update_data = {
                        "status": "rejected",
                        "rejected_at": datetime.now().isoformat()
                    }
                    
                    if request.reason:
                        update_data["rejection_reason"] = request.reason
                    
                    client.table("url_suggestions").update(update_data).eq("id", suggestion_id).execute()
                    
                    rejected_urls.append({
                        "id": suggestion_id,
                        "url": suggestion["url"]
                    })
                    
                    # Learn from user feedback
                    await url_decision_agent.learn_from_user_feedback(
                        suggestion["url"],
                        suggestion["recommended_action"],
                        "rejected",
                        request.reason or "User manually rejected suggestion"
                    )
                    
                except Exception as e:
                    logger.error(f"‚ùå Error rejecting suggestion {suggestion_id}: {e}")
                    failed_rejections.append({"id": suggestion_id, "error": str(e)})
            
            safe_set_attribute(span, "rejected_count", len(rejected_urls))
            safe_set_attribute(span, "failed_count", len(failed_rejections))
            
            return {
                "success": True,
                "rejected_count": len(rejected_urls),
                "failed_count": len(failed_rejections),
                "rejected_urls": rejected_urls,
                "failed_rejections": failed_rejections,
                "message": f"Rejected {len(rejected_urls)} URLs, {len(failed_rejections)} failed"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in rejection process: {e}")
            raise HTTPException(status_code=500, detail="Failed to reject URL suggestions")


@router.post("/analyze")
async def analyze_url_manually(request: URLAnalysisRequest):
    """
    Manually analyze a URL for potential addition to the knowledge base.
    
    This endpoint allows users to test the URL analysis system or analyze
    specific URLs that weren't automatically detected.
    """
    with safe_span("analyze_url_manually") as span:
        safe_set_attribute(span, "url", request.url)
        safe_set_attribute(span, "context", request.context)
        
        try:
            url_detection_service = get_url_detection_service()
            
            # Perform analysis
            analysis = await url_detection_service.analyze_url(
                request.url, 
                f"manual_analysis:{request.context}"
            )
            
            safe_set_attribute(span, "overall_score", analysis.overall_score)
            safe_set_attribute(span, "recommended_action", analysis.recommended_action)
            
            return {
                "success": True,
                "analysis": {
                    "url": analysis.url,
                    "domain": analysis.domain,
                    "relevance_score": analysis.relevance_score,
                    "content_quality_score": analysis.content_quality_score,
                    "domain_reputation_score": analysis.domain_reputation_score,
                    "overall_score": analysis.overall_score,
                    "reasoning": analysis.reasoning,
                    "recommended_action": analysis.recommended_action,
                    "tags": analysis.tags,
                    "detected_at": analysis.detected_at.isoformat()
                },
                "message": f"Analysis complete: {analysis.recommended_action} (score: {analysis.overall_score:.2f})"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing URL {request.url}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to analyze URL: {str(e)}")


@router.get("/analytics")
async def get_url_detection_analytics(
    days: int = Query(7, ge=1, le=90, description="Number of days to analyze"),
    group_by: str = Query("day", description="Grouping: day, hour, domain, action")
):
    """
    Get analytics data for URL detection system performance.
    """
    with safe_span("get_url_detection_analytics") as span:
        safe_set_attribute(span, "days", days)
        safe_set_attribute(span, "group_by", group_by)
        
        try:
            client = get_supabase_client()
            
            # Get analytics data
            from datetime import timedelta
            start_date = datetime.now() - timedelta(days=days)
            
            result = client.table("url_detection_analytics").select(
                "*"
            ).gte("event_timestamp", start_date.isoformat()).execute()
            
            if not result.data:
                return {"analytics": [], "summary": {}}
            
            # Process analytics data based on grouping
            analytics_data = result.data
            
            if group_by == "domain":
                # Group by domain
                domain_stats = {}
                for event in analytics_data:
                    domain = event["domain"]
                    if domain not in domain_stats:
                        domain_stats[domain] = {"detected": 0, "auto_added": 0, "suggested": 0, "rejected": 0}
                    domain_stats[domain][event["event_type"]] = domain_stats[domain].get(event["event_type"], 0) + 1
                
                analytics = [{"domain": domain, **stats} for domain, stats in domain_stats.items()]
                
            elif group_by == "action":
                # Group by event type
                action_stats = {}
                for event in analytics_data:
                    event_type = event["event_type"]
                    action_stats[event_type] = action_stats.get(event_type, 0) + 1
                
                analytics = [{"action": action, "count": count} for action, count in action_stats.items()]
                
            else:
                # Group by day (default)
                from collections import defaultdict
                daily_stats = defaultdict(lambda: {"detected": 0, "analyzed": 0, "auto_added": 0, "suggested": 0})
                
                for event in analytics_data:
                    date_str = event["event_timestamp"][:10]  # YYYY-MM-DD
                    daily_stats[date_str][event["event_type"]] += 1
                
                analytics = [{"date": date, **stats} for date, stats in daily_stats.items()]
                analytics.sort(key=lambda x: x["date"])
            
            # Calculate summary statistics
            total_detected = sum(1 for e in analytics_data if e["event_type"] == "detected")
            total_analyzed = sum(1 for e in analytics_data if e["event_type"] == "analyzed")
            total_auto_added = sum(1 for e in analytics_data if e["event_type"] == "auto_added")
            total_suggested = sum(1 for e in analytics_data if e["event_type"] == "suggested")
            
            summary = {
                "total_detected": total_detected,
                "total_analyzed": total_analyzed,
                "total_auto_added": total_auto_added,
                "total_suggested": total_suggested,
                "auto_add_rate": total_auto_added / max(total_analyzed, 1),
                "suggestion_rate": total_suggested / max(total_analyzed, 1)
            }
            
            safe_set_attribute(span, "events_analyzed", len(analytics_data))
            
            return {
                "success": True,
                "analytics": analytics,
                "summary": summary,
                "period": f"Last {days} days"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error getting analytics: {e}")
            raise HTTPException(status_code=500, detail="Failed to fetch analytics data")


@router.get("/settings", response_model=URLDetectionSettingsResponse)
async def get_url_detection_settings():
    """Get current URL detection system settings."""
    try:
        client = get_supabase_client()
        
        # Get global settings
        result = client.table("url_detection_settings").select(
            "setting_value"
        ).eq("setting_key", "global_config").is_("user_id", None).is_(
            "project_id", None
        ).single().execute()
        
        if result.data:
            config = result.data["setting_value"]
            return URLDetectionSettingsResponse(**config)
        else:
            # Return default settings
            return URLDetectionSettingsResponse(
                enabled=True,
                auto_add_threshold=0.85,
                suggest_threshold=0.6,
                ignore_threshold=0.3,
                excluded_domains=["localhost", "127.0.0.1", "0.0.0.0"],
                preferred_domains=["github.com", "docs.python.org", "stackoverflow.com"]
            )
            
    except Exception as e:
        logger.error(f"‚ùå Error getting settings: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch settings")


@router.post("/settings")
async def update_url_detection_settings(request: UpdateSettingsRequest):
    """Update URL detection system settings."""
    try:
        client = get_supabase_client()
        
        # Get current settings
        result = client.table("url_detection_settings").select(
            "setting_value"
        ).eq("setting_key", "global_config").is_("user_id", None).is_(
            "project_id", None
        ).single().execute()
        
        current_config = result.data["setting_value"] if result.data else {}
        
        # Update with new values
        if request.enabled is not None:
            current_config["enabled"] = request.enabled
        if request.auto_add_threshold is not None:
            current_config["auto_add_threshold"] = request.auto_add_threshold
        if request.suggest_threshold is not None:
            current_config["suggest_threshold"] = request.suggest_threshold
        if request.ignore_threshold is not None:
            current_config["ignore_threshold"] = request.ignore_threshold
        if request.excluded_domains is not None:
            current_config["excluded_domains"] = request.excluded_domains
        if request.preferred_domains is not None:
            current_config["preferred_domains"] = request.preferred_domains
        
        # Update in database
        client.table("url_detection_settings").upsert({
            "user_id": None,
            "project_id": None,
            "setting_key": "global_config",
            "setting_value": current_config
        }).execute()
        
        return {
            "success": True,
            "settings": current_config,
            "message": "Settings updated successfully"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error updating settings: {e}")
        raise HTTPException(status_code=500, detail="Failed to update settings")